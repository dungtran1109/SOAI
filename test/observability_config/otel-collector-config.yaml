receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

  hostmetrics:
    collection_interval: 10s
    scrapers:
      cpu: {}
      memory: {}
      disk: {}
      filesystem: {}
      network: {}
      load: {}

  prometheus:
    config:
      scrape_configs:
        - job_name: 'soai_services'
          metrics_path: /metrics
          static_configs:
            - targets: 
              - 'soai_gen_ai_provider:8004'
              - 'soai_recruitment_agent:8003'
          relabel_configs:
            - source_labels: [__address__]
              regex: '(.*):.*'
              target_label: instance
              replacement: '$${1}'
            - source_labels: [__address__]
              target_label: service
              regex: 'genai:.*'
              replacement: 'gen_ai_provider'
            - source_labels: [__address__]
              target_label: service
              regex: 'recruitment:.*'
              replacement: 'recruitment_agent'

        - job_name: 'authentication_service'
          metrics_path: /actuator/prometheus
          static_configs:
            - targets:
              - 'soai_authentication:9090'
          relabel_configs:
            - source_labels: [__address__]
              regex: '(.*):.*'
              target_label: instance
              replacement: '$${1}'
            - source_labels: [__address__]
              target_label: service
              regex: 'authentication:.*'
              replacement: 'authentication_service'

exporters:
  clickhouse:
    endpoint: tcp://clickhouse:9000
    username: soai_user
    password: soai_password
    database: otel
    create_schema: true
    logs_table_name: otel_logs
    traces_table_name: otel_traces
    metrics_table_name: otel_metrics
    timeout: 5s
    # TTL configuration - auto-delete old data to prevent unbounded growth
    # Applies to all tables (logs, traces, metrics)
    ttl: 168h  # 7 days retention
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 300s

processors:
  memory_limiter:
    check_interval: 1s
    limit_mib: 1000
    spike_limit_mib: 200

  # Filter out noisy health check and probe logs
  filter/logs:
    logs:
      exclude:
        match_type: regexp
        bodies:
          - '.*"GET /health.*'
          - '.*"GET /api/v1/.*/health.*'
          - '.*readiness.*'
          - '.*liveness.*'
          - '.*healthcheck.*'
          - '.*GET /metrics.*'

  # Sample DEBUG level logs (keep only 10%)
  # INFO, WARN, ERROR logs are always kept
  probabilistic_sampler/logs:
    sampling_percentage: 100  # Base rate for all logs

  # Tail sampling for traces - keep error traces and sample others
  tail_sampling:
    decision_wait: 10s
    num_traces: 100
    expected_new_traces_per_sec: 10
    policies:
      - name: error-policy
        type: status_code
        status_code: {status_codes: [ERROR]}
      - name: probabilistic-policy
        type: probabilistic
        probabilistic: {sampling_percentage: 20}

  k8sattributes:
    passthrough: true # Only keep this for docker environment
    extract:
      metadata:
        - k8s.namespace.name
        - k8s.deployment.name
        - k8s.statefulset.name
        - k8s.daemonset.name
        - k8s.cronjob.name
        - k8s.job.name
        - k8s.node.name
        - k8s.pod.name
        - k8s.pod.uid
        - k8s.pod.start_time
        - container.image.name
        - container.image.tag
        - k8s.container.name
    pod_association:
      - sources:
          - from: resource_attribute
            name: k8s.pod.ip
      - sources:
          - from: resource_attribute
            name: k8s.pod.uid
      - sources:
          - from: connection

  resource:
    attributes:
      - action: insert
        from_attribute: k8s.pod.uid
        key: service.instance.id

  batch:
    send_batch_size: 5000
    timeout: 10s

  # Smaller batch for logs to reduce memory pressure
  batch/logs:
    send_batch_size: 2000
    timeout: 5s

  transform:
    error_mode: ignore
    metric_statements:
      - context: datapoint
        statements:
          - set(attributes["k8s_cluster_name"], resource.attributes["k8s.cluster.name"])
          - set(attributes["namespace"], resource.attributes["k8s.namespace.name"])
          - set(attributes["container"], resource.attributes["k8s.container.name"])
          - set(attributes["pod"], resource.attributes["k8s.pod.name"])

  transform/logs:
    error_mode: ignore
    log_statements:
      - context: resource
        statements:
          - keep_keys(attributes, ["k8s.namespace.name", "k8s.container.name", "k8s.pod.name"])
      - context: log
        statements:
          - set(attributes["namespace"], resource.attributes["k8s.namespace.name"])
          - set(attributes["container"], resource.attributes["k8s.container.name"])
          - set(attributes["pod"], resource.attributes["k8s.pod.name"])
          - set(attributes["message"], body)
          - set(body, "")
          - delete_key(resource.attributes, "k8s.namespace.name")
          - delete_key(resource.attributes, "k8s.container.name")
          - delete_key(resource.attributes, "k8s.pod.name")

service:
  telemetry:
    metrics:
      readers:
        - pull:
            exporter:
              prometheus:
                host: 0.0.0.0
                port: 8888
    logs:
      encoding: json

  pipelines:
    metrics/primary:
      receivers: [otlp, hostmetrics, prometheus]
      processors: [memory_limiter, k8sattributes, resource, transform, batch]
      exporters: [clickhouse]

    traces:
      receivers: [otlp]
      processors: [memory_limiter, k8sattributes, tail_sampling, batch]
      exporters: [clickhouse]

    logs:
      receivers: [otlp]
      processors: [memory_limiter, filter/logs, k8sattributes, transform/logs, batch/logs]
      exporters: [clickhouse]
